{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import sys; sys.path.append(\"..\") # Adds parent directory to python modules path.\n",
    "from topdown_parsers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Race Key ***\n",
    "\n",
    "    White: 1\n",
    "    Black: 2\n",
    "    American Indian or Alaska native: 3\n",
    "    Asian : 4\n",
    "    Hawaiian : 5\n",
    "    Other, Multiracial: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_hh_dirname = \"./with_hhs/\"\n",
    "without_hh_dirname = \"./without_hhs/\"\n",
    "dallas_filename = \"DALLAS.dat\"\n",
    "precinct_assignments_fp = \"block_prec_assign.csv\"\n",
    "state_id = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precinct_assignments(precinct_assignments_fp):\n",
    "    \"\"\" Reads `precinct_assignments_fp` as Pandas DataFrame, cleans it and returns it.\n",
    "    \"\"\"\n",
    "    precinct_assignments = pd.read_csv(precinct_assignments_fp)\n",
    "    precinct_assignments.columns = [\"GEOID10\", \"Precinct\"]\n",
    "    precinct_assignments[\"GEOID10\"] = precinct_assignments[\"GEOID10\"].astype(str)\n",
    "    return precinct_assignments\n",
    "\n",
    "def extract_from_zip(zipfile_fp, \n",
    "                     destination_fp):\n",
    "    \"\"\" Unzips the file `zipfile_fp` at the filepath location `destination_fp`.\n",
    "    \"\"\"\n",
    "    print(\"Extracting {}\".format(zipfile_fp))\n",
    "    \n",
    "    with zipfile.ZipFile(zipfile_fp, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_fp)\n",
    "        \n",
    "def clean_df(df):\n",
    "    \"\"\" Some simple cleanups on `df` to ready it to make ER csvs.\n",
    "    \"\"\"\n",
    "    df[\"Enumdist\"] = df[\"Enumdist\"].astype(str).str.pad(width=11, side='left', fillchar='0')\n",
    "    df[\"County\"] = df[\"County\"].astype(str).str.pad(width=3, side='left', fillchar='0')\n",
    "    df[\"GEOID10\"] = df[\"State\"].astype(str) + df[\"County\"] + df[\"Enumdist\"]\n",
    "    df[\"GEOID10\"] = df[\"GEOID10\"].str[:11] + df[\"GEOID10\"].str[-4:]\n",
    "    df = df.drop(columns=[\"State\"])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rename_cols(df, string):\n",
    "    \"\"\" Renames each column in `df` named Run_x as (x-1)_`string`_noise. \n",
    "        Eg if string is `HVAP` and a column is called Run_1, it would be renamed as \n",
    "        \"0_HVAP_noise\".\n",
    "    \"\"\"\n",
    "    df = df.fillna(0)\n",
    "    for col_name in df.columns:\n",
    "        if col_name[:3] == \"Run\":\n",
    "            new_name = str(int(col_name[4:]) - 1) + \"_{}_noise\".format(string)\n",
    "            df = df.rename(columns={col_name: new_name})\n",
    "    return df\n",
    "\n",
    "def build_er_df(dir_name, state_id, filename, precinct_assignments_fp):\n",
    "    \"\"\" Builds a CSV that can be fed into plot_elect_grid() to easily make ER plots.\n",
    "        \n",
    "        Args:\n",
    "            dir_name (str) : Filepath where `filename` exists.\n",
    "            state_id (int) : FIPS code of state the runs were run on.\n",
    "            filename (str) : Name of MDF file in `dir_name` to open.\n",
    "            precinct_assignments_fp (str) : Filepath to where the precinct_assignments file is.\n",
    "    \"\"\"\n",
    "    hvap = collect_by_enumdist(dir_name, state_id, filename, hisp=True, vap=True)\n",
    "    wvap = collect_by_enumdist(dir_name, state_id, filename, race=1, vap=True)\n",
    "    bvap = collect_by_enumdist(dir_name, state_id, filename, race=2, vap=True)\n",
    "    vap = collect_by_enumdist(dir_name, state_id, filename, vap=True)\n",
    "    \n",
    "    # clean\n",
    "    hvap = clean_df(hvap)\n",
    "    wvap = clean_df(wvap)\n",
    "    bvap = clean_df(bvap)\n",
    "    vap = clean_df(vap)\n",
    "    \n",
    "    # merge with precincts\n",
    "    precinct_assignments = get_precinct_assignments(precinct_assignments_fp)\n",
    "    \n",
    "    hvap = hvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    wvap = wvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    bvap = bvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    vap = vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    \n",
    "    # rename\n",
    "    hvap= rename_cols(hvap, \"HVAP\")\n",
    "    wvap= rename_cols(wvap, \"WVAP\")\n",
    "    bvap= rename_cols(bvap, \"BVAP\")\n",
    "    vap= rename_cols(vap, \"VAP\")\n",
    "    \n",
    "    # groupby Precincts\n",
    "    hvap = hvap.groupby(\"Precinct\").sum()\n",
    "    wvap = wvap.groupby(\"Precinct\").sum()\n",
    "    bvap = bvap.groupby(\"Precinct\").sum()\n",
    "    vap = vap.groupby(\"Precinct\").sum()\n",
    "    \n",
    "    df_merged = reduce(lambda left,right: pd.merge(left, right, how='outer', left_index=True, right_index=True), \n",
    "                       [hvap, wvap, bvap, vap])\n",
    "    return df_merged\n",
    "\n",
    "def get_pops(dir_name, state_id, filename):\n",
    "    \"\"\" Gets the population from each file `filename` in the \"output_\" dirs in `dirname`, \n",
    "        cleans it and returns it in a Pandas DataFrame.\n",
    "        State ID is the FIPS code of the state where the runs are run on.\n",
    "    \"\"\"\n",
    "    pops = collect_by_enumdist(dir_name, state_id, filename)\n",
    "    pops = clean_df(pops)\n",
    "    return pops\n",
    "\n",
    "def save_populations_at_block_level(runs_dirname, \n",
    "                                    zips_arr, \n",
    "                                    dallas_filename,\n",
    "                                    state_id=48,\n",
    "                                    save_filename=None):\n",
    "    \"\"\" Save the population at the blocks level for the runs in `zips_arr`.\n",
    "        \n",
    "        Args:\n",
    "            runs_dirname (str) : directory where the files in `zips_arr` are stored.\n",
    "            zips_arr (list str): list of zip files that contain the runs \n",
    "            state_id     (int) : FIPS code of state the runs are from. Defaults to 48 for TX\n",
    "            dallas_filename (str) : Name of Dallas MDF file in each output dir\n",
    "            save_filename (str) : Filename to save the populations file at.\n",
    "    \"\"\"\n",
    "    for run in zips_arr:\n",
    "        filename = run[:-8]\n",
    "        extract_from_zip(runs_dirname + run, runs_dirname)               \n",
    "        tot_pops = get_pops(runs_dirname + runs_dirname + run[:-4], state_id, dallas_filename)\n",
    "        tot_pops.to_csv(filename + \"_block_pops.csv\")\n",
    "        os.system(\"rm -r \" + runs_dirname + runs_dirname)\n",
    "        \n",
    "def build_csvs(runs_dirname, dallas_filename, precinct_assignments_fp, state_id):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(runs_dirname):\n",
    "        for file in files:\n",
    "            if file[-4:] == \".zip\":\n",
    "                texas_fp = root + file\n",
    "                dirname = texas_fp[:-4]\n",
    "                extract_from_zip(texas_fp, root)               \n",
    "                df = build_er_df(runs_dirname, state_id, dallas_filename, precinct_assignments_fp)\n",
    "                save_filename = file[:-8]\n",
    "                df.to_csv(save_filename + \".csv\")\n",
    "                print(\"Deleting {}\".format(runs_dirname + runs_dirname[:-1]))\n",
    "                os.system(\"rm -r \" + runs_dirname + runs_dirname[:-1])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_2.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_1.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_0pt25.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_bottom_0pt25.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_1.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_2.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_0pt5.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_0pt5.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_0pt25.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_2.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_0pt5.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_bottom_0pt5.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_0pt25.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_1.ini.zip\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_csvs(with_hh_dirname, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./without_hhs/TEXAS_STUB_bottom_0pt25.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_eq_0pt5.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_top_0pt25.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_eq_1.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_top_1.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_top_0pt5.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_mid_0pt5.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_eq_2.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_top_2.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_mid_0pt25.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_eq_0pt25.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_mid_2.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_bottom_0pt5.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n",
      "Extracting ./without_hhs/TEXAS_STUB_mid_1.ini.zip\n",
      "Deleting ./without_hhs/./without_hhs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_csvs(without_hh_dirname, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting with_hhs/TEXAS_STUB_HH_mid_1.ini.zip\n",
      "Extracting with_hhs/TEXAS_STUB_HH_eq_1.ini.zip\n",
      "Extracting with_hhs/TEXAS_STUB_HH_top_1.ini.zip\n"
     ]
    }
   ],
   "source": [
    "runs_with_hhs = ['TEXAS_STUB_HH_mid_1.ini.zip',\n",
    "                 'TEXAS_STUB_HH_eq_1.ini.zip',\n",
    "                 'TEXAS_STUB_HH_top_1.ini.zip']\n",
    "\n",
    "save_populations_at_block_level(\"with_hhs/\", runs_with_hhs, dallas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./remaining_runs/TEXAS_STUB_HH_bottom_2.ini.zip\n",
      "Deleting ./remaining_runs/./remaining_runs\n",
      "\n",
      "Extracting ./remaining_runs/TEXAS_STUB_HH_bottom_1.ini.zip\n",
      "Deleting ./remaining_runs/./remaining_runs\n",
      "\n",
      "Extracting ./remaining_runs/TEXAS_STUB_bottom_1.ini.zip\n",
      "Deleting ./remaining_runs/./remaining_runs\n",
      "\n",
      "Extracting ./remaining_runs/TEXAS_STUB_bottom_2.ini.zip\n",
      "Deleting ./remaining_runs/./remaining_runs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remaining_runs_dir = \"./remaining_runs/\"\n",
    "build_csvs(remaining_runs_dir, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./remaining_runs/TEXAS_STUB_HH_bottom_1.ini.zip\n",
      "Extracting ./remaining_runs/TEXAS_STUB_eq_1_eps_100.ini.zip\n",
      "Extracting ./remaining_runs/TEXAS_STUB_eq_1_not_detailed.ini.zip\n",
      "Extracting ./remaining_runs/TEXAS_STUB_bottom_1.ini.zip\n",
      "Extracting ./remaining_runs/TEXAS_STUB_mid_1_bg_weighted.ini.zip\n"
     ]
    }
   ],
   "source": [
    "runs_without_hhs = ['TEXAS_STUB_HH_bottom_1.ini.zip',\n",
    "                     'TEXAS_STUB_eq_1_eps_100.ini.zip',\n",
    "                     'TEXAS_STUB_eq_1_not_detailed.ini.zip',\n",
    "                     'TEXAS_STUB_bottom_1.ini.zip',\n",
    "                     'TEXAS_STUB_mid_1_bg_weighted.ini.zip']\n",
    "                     \n",
    "save_populations_at_block_level(\"./remaining_runs/\", runs_without_hhs, dallas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEXAS_STUB_HH_eq_2.ini.zip',\n",
       " '.DS_Store',\n",
       " 'TEXAS_STUB_HH_bottom_2.ini.zip',\n",
       " 'TEXAS_STUB_HH_mid_1.ini.zip',\n",
       " 'TEXAS_STUB_HH_eq_0pt25.ini.zip',\n",
       " 'TEXAS_STUB_HH_bottom_0pt25.ini.zip',\n",
       " 'TEXAS_STUB_HH_eq_1.ini.zip',\n",
       " 'TEXAS_STUB_HH_mid_2.ini.zip',\n",
       " 'TEXAS_STUB_HH_top_0pt5.ini.zip',\n",
       " 'TEXAS_STUB_HH_mid_0pt5.ini.zip',\n",
       " 'TEXAS_STUB_HH_top_0pt25.ini.zip',\n",
       " 'TEXAS_STUB_HH_top_2.ini.zip',\n",
       " 'TEXAS_STUB_HH_eq_0pt5.ini.zip',\n",
       " 'TEXAS_STUB_HH_bottom_0pt5.ini.zip',\n",
       " 'TEXAS_STUB_HH_mid_0pt25.ini.zip',\n",
       " 'TEXAS_STUB_HH_top_1.ini.zip']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./with_hhs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
