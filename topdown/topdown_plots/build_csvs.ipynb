{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import sys; sys.path.append(\"..\") # Adds parent directory to python modules path.\n",
    "from topdown_parsers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Race Key ***\n",
    "\n",
    "    White: 1\n",
    "    Black: 2\n",
    "    American Indian or Alaska native: 3\n",
    "    Asian : 4\n",
    "    Hawaiian : 5\n",
    "    Other, Multiracial: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_hh_dirname = \"./with_hhs/\"\n",
    "without_hh_dirname = \"./without_hhs/\"\n",
    "dallas_filename = \"DALLAS.dat\"\n",
    "precinct_assignments_fp = \"block_prec_assign.csv\"\n",
    "state_id = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precinct_assignments(precinct_assignments_fp):\n",
    "    \"\"\" Reads `precinct_assignments_fp` as Pandas DataFrame, cleans it and returns it.\n",
    "    \"\"\"\n",
    "    precinct_assignments = pd.read_csv(precinct_assignments_fp)\n",
    "    precinct_assignments.columns = [\"GEOID10\", \"Precinct\"]\n",
    "    precinct_assignments[\"GEOID10\"] = precinct_assignments[\"GEOID10\"].astype(str)\n",
    "    return precinct_assignments\n",
    "\n",
    "def extract_from_zip(zipfile_fp, \n",
    "                     destination_fp):\n",
    "    \"\"\" Unzips the file `zipfile_fp` at the filepath location `destination_fp`.\n",
    "    \"\"\"\n",
    "    print(\"Extracting {}\".format(zipfile_fp))\n",
    "    \n",
    "    with zipfile.ZipFile(zipfile_fp, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_fp)\n",
    "        \n",
    "def clean_df(df):\n",
    "    \"\"\" Some simple cleanups on `df` to ready it to make ER csvs.\n",
    "    \"\"\"\n",
    "    df[\"Enumdist\"] = df[\"Enumdist\"].astype(str).str.pad(width=11, side='left', fillchar='0')\n",
    "    df[\"County\"] = df[\"County\"].astype(str).str.pad(width=3, side='left', fillchar='0')\n",
    "    df[\"GEOID10\"] = df[\"State\"].astype(str) + df[\"County\"] + df[\"Enumdist\"]\n",
    "    df[\"GEOID10\"] = df[\"GEOID10\"].str[:11] + df[\"GEOID10\"].str[-4:]\n",
    "    df = df.drop(columns=[\"State\"])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rename_cols(df, string):\n",
    "    \"\"\" Renames each column in `df` named Run_x as (x-1)_`string`_noise. \n",
    "        Eg if string is `HVAP` and a column is called Run_1, it would be renamed as \n",
    "        \"0_HVAP_noise\".\n",
    "    \"\"\"\n",
    "    df = df.fillna(0)\n",
    "    for col_name in df.columns:\n",
    "        if col_name[:3] == \"Run\":\n",
    "            new_name = str(int(col_name[4:]) - 1) + \"_{}_noise\".format(string)\n",
    "            df = df.rename(columns={col_name: new_name})\n",
    "    return df\n",
    "\n",
    "def build_er_df(dir_name, state_id, filename, precinct_assignments_fp):\n",
    "    \"\"\" Builds a CSV that can be fed into plot_elect_grid() to easily make ER plots.\n",
    "        \n",
    "        Args:\n",
    "            dir_name (str) : Filepath where `filename` exists.\n",
    "            state_id (int) : FIPS code of state the runs were run on.\n",
    "            filename (str) : Name of MDF file in `dir_name` to open.\n",
    "            precinct_assignments_fp (str) : Filepath to where the precinct_assignments file is.\n",
    "    \"\"\"\n",
    "    print(\"Collecting races...\")\n",
    "    hisp = collect_by_enumdist(dir_name, state_id, filename, hisp=True, vap=False)\n",
    "    nh_white = collect_by_enumdist(dir_name, state_id, filename, race=1, vap=False)\n",
    "    nh_black = collect_by_enumdist(dir_name, state_id, filename, race=2, vap=False)\n",
    "    nh_amin = collect_by_enumdist(dir_name, state_id, filename, race=3, vap=False)\n",
    "    nh_asian = collect_by_enumdist(dir_name, state_id, filename, race=4, vap=False)\n",
    "    nh_hawaiian = collect_by_enumdist(dir_name, state_id, filename, race=5, vap=False)\n",
    "    nh_other = collect_by_enumdist(dir_name, state_id, filename, race=6, vap=False)\n",
    "    \n",
    "    hvap = collect_by_enumdist(dir_name, state_id, filename, hisp=True, vap=True)\n",
    "    wvap = collect_by_enumdist(dir_name, state_id, filename, race=1, vap=True)\n",
    "    bvap = collect_by_enumdist(dir_name, state_id, filename, race=2, vap=True)\n",
    "    amin_vap = collect_by_enumdist(dir_name, state_id, filename, race=3, vap=True)\n",
    "    asian_vap = collect_by_enumdist(dir_name, state_id, filename, race=4, vap=True)\n",
    "    hawaiian_vap = collect_by_enumdist(dir_name, state_id, filename, race=5, vap=True)\n",
    "    other_vap = collect_by_enumdist(dir_name, state_id, filename, race=6, vap=True)\n",
    "    \n",
    "    vap = collect_by_enumdist(dir_name, state_id, filename, vap=True)\n",
    "    \n",
    "    print(\"Cleaning...\")\n",
    "    # clean\n",
    "    [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, \n",
    "     hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = [clean_df(df) for df in [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]]\n",
    "    \n",
    "    print(\"Merging with precincts...\")\n",
    "    # merge with precincts\n",
    "    precinct_assignments = get_precinct_assignments(precinct_assignments_fp)\n",
    "    \n",
    "    hisp = hisp.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    nh_white = nh_white.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    nh_black = nh_black.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    nh_amin = nh_amin.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    nh_asian = nh_asian.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    nh_hawaiian = nh_hawaiian.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    nh_other = nh_other.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    \n",
    "    hvap = hvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    wvap = wvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    bvap = bvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    amin_vap = amin_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    asian_vap = asian_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    hawaiian_vap = hawaiian_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    other_vap = other_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    \n",
    "    vap = vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "    \n",
    "    # rename\n",
    "    def rename_multiple_dfs(dfs, names):\n",
    "        assert(len(dfs) == len(names))\n",
    "        \n",
    "        new_dfs = []\n",
    "        for i in range(len(dfs)):\n",
    "            new_dfs.append(rename_cols(dfs[i], names[i]))\n",
    "        \n",
    "        return new_dfs\n",
    "    \n",
    "    dfs = [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]\n",
    "    \n",
    "    names = [\"HISP\", \"NHWHITE\", \"NHBLACK\", \"NHAMIN\", \"NHASIAN\", \"NHHAWAIIAN\", \"NHOTHER\", \n",
    "             \"HVAP\", \"WVAP\", \"BVAP\", \"AMINVAP\", \"ASIANVAP\", \"HAWAIIANVAP\", \"OTHERVAP\", \"VAP\"]\n",
    "    \n",
    "    print(\"Renaming cols...\")\n",
    "    [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = rename_multiple_dfs(dfs, names)\n",
    "    \n",
    "    \n",
    "    # groupby Precincts\n",
    "    dfs = [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]\n",
    "    \n",
    "    def groupby_precincts(dfs):\n",
    "        new_dfs = []\n",
    "        for df in dfs:\n",
    "            new_dfs.append(df.groupby(\"Precinct\").sum())\n",
    "        return new_dfs\n",
    "            \n",
    "    print(\"Grouping by precincts..\")\n",
    "    [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = groupby_precincts(dfs)\n",
    "    \n",
    "    dfs = [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]\n",
    "    \n",
    "    print(\"The big merge...\")\n",
    "    df_merged = reduce(lambda left,right: pd.merge(left, right, how='outer', left_index=True, right_index=True), \n",
    "                       dfs)\n",
    "    return df_merged\n",
    "\n",
    "def get_pops(dir_name, state_id, filename):\n",
    "    \"\"\" Gets the population from each file `filename` in the \"output_\" dirs in `dirname`, \n",
    "        cleans it and returns it in a Pandas DataFrame.\n",
    "        State ID is the FIPS code of the state where the runs are run on.\n",
    "    \"\"\"\n",
    "    pops = collect_by_enumdist(dir_name, state_id, filename)\n",
    "    pops = clean_df(pops)\n",
    "    return pops\n",
    "\n",
    "def save_populations_at_block_level(runs_dirname, \n",
    "                                    zips_arr, \n",
    "                                    dallas_filename,\n",
    "                                    state_id=48,\n",
    "                                    save_filename=None):\n",
    "    \"\"\" Save the population at the blocks level for the runs in `zips_arr`.\n",
    "        \n",
    "        Args:\n",
    "            runs_dirname (str) : directory where the files in `zips_arr` are stored.\n",
    "            zips_arr (list str): list of zip files that contain the runs \n",
    "            state_id     (int) : FIPS code of state the runs are from. Defaults to 48 for TX\n",
    "            dallas_filename (str) : Name of Dallas MDF file in each output dir\n",
    "            save_filename (str) : Filename to save the populations file at.\n",
    "    \"\"\"\n",
    "    for run in zips_arr:\n",
    "        filename = run[:-8]\n",
    "        extract_from_zip(runs_dirname + run, runs_dirname)               \n",
    "        tot_pops = get_pops(runs_dirname + runs_dirname + run[:-4], state_id, dallas_filename)\n",
    "        tot_pops.to_csv(filename + \"_block_pops.csv\")\n",
    "        os.system(\"rm -r \" + runs_dirname + runs_dirname)\n",
    "        \n",
    "def build_csvs(runs_dirname, dallas_filename, precinct_assignments_fp, state_id):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(runs_dirname):\n",
    "        for file in files:\n",
    "            if file[-4:] == \".zip\":\n",
    "                texas_fp = root + file\n",
    "                dirname = texas_fp[:-4]\n",
    "                extract_from_zip(texas_fp, root)               \n",
    "                df = build_er_df(runs_dirname, state_id, dallas_filename, precinct_assignments_fp)\n",
    "                save_filename = file[:-8]\n",
    "                df.to_csv(save_filename + \".csv\")\n",
    "                print(\"Deleting {}\".format(runs_dirname + runs_dirname[:-1]))\n",
    "                os.system(\"rm -r \" + runs_dirname + runs_dirname[:-1])\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_2.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_bottom_2.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_1.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_0pt25.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_bottom_0pt25.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_bottom_1.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_1.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_2.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_0pt5.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_0pt5.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_0pt25.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_2.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_eq_0pt5.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_bottom_0pt5.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_mid_0pt25.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n",
      "Extracting ./with_hhs/TEXAS_STUB_HH_top_1.ini.zip\n",
      "Collecting races...\n",
      "Cleaning...\n",
      "Merging with precincts...\n",
      "Renaming cols...\n",
      "Grouping by precincts..\n",
      "The big merge...\n",
      "Deleting ./with_hhs/./with_hhs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_csvs(with_hh_dirname, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_csvs(without_hh_dirname, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_with_hhs = ['TEXAS_STUB_HH_mid_1.ini.zip',\n",
    "                 'TEXAS_STUB_HH_eq_1.ini.zip',\n",
    "                 'TEXAS_STUB_HH_top_1.ini.zip']\n",
    "\n",
    "save_populations_at_block_level(\"with_hhs/\", runs_with_hhs, dallas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_runs_dir = \"./remaining_runs/\"\n",
    "build_csvs(remaining_runs_dir, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_without_hhs = ['TEXAS_STUB_HH_bottom_1.ini.zip',\n",
    "                     'TEXAS_STUB_eq_1_eps_100.ini.zip',\n",
    "                     'TEXAS_STUB_eq_1_not_detailed.ini.zip',\n",
    "                     'TEXAS_STUB_bottom_1.ini.zip',\n",
    "                     'TEXAS_STUB_mid_1_bg_weighted.ini.zip']\n",
    "                     \n",
    "save_populations_at_block_level(\"./remaining_runs/\", runs_without_hhs, dallas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./with_hhs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
