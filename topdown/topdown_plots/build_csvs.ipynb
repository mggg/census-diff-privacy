{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import sys; sys.path.append(\"..\") # Adds parent directory to python modules path.\n",
    "from topdown_parsers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Race Key ***\n",
    "\n",
    "    White: 1\n",
    "    Black: 2\n",
    "    American Indian or Alaska native: 3\n",
    "    Asian : 4\n",
    "    Hawaiian : 5\n",
    "    Other, Multiracial: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_hh_dirname = \"./with_hhs/\"\n",
    "without_hh_dirname = \"./without_hhs/\"\n",
    "# dallas_filename = \"DALLAS.dat\"\n",
    "# precinct_assignments_fp = \"block_prec_assign.csv\"\n",
    "# state_id = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precinct_assignments(precinct_assignments_fp):\n",
    "    \"\"\" Reads `precinct_assignments_fp` as Pandas DataFrame, cleans it and returns it.\n",
    "    \"\"\"\n",
    "    precinct_assignments = pd.read_csv(precinct_assignments_fp)\n",
    "    precinct_assignments.columns = [\"GEOID10\", \"Precinct\"]\n",
    "    precinct_assignments[\"GEOID10\"] = precinct_assignments[\"GEOID10\"].astype(str)\n",
    "    return precinct_assignments\n",
    "        \n",
    "def clean_df(df):\n",
    "    \"\"\" Some simple cleanups on `df` to ready it to make ER csvs.\n",
    "    \"\"\"\n",
    "    df[\"Enumdist\"] = df[\"Enumdist\"].astype(str).str.pad(width=11, side='left', fillchar='0')\n",
    "    df[\"County\"] = df[\"County\"].astype(str).str.pad(width=3, side='left', fillchar='0')\n",
    "    df[\"GEOID10\"] = df[\"State\"].astype(str) + df[\"County\"] + df[\"Enumdist\"]\n",
    "    df[\"GEOID10\"] = df[\"GEOID10\"].str[:11] + df[\"GEOID10\"].str[-4:]\n",
    "    df = df.drop(columns=[\"State\"])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rename_cols(df, string):\n",
    "    \"\"\" Renames each column in `df` named Run_x as (x-1)_`string`_noise. \n",
    "        Eg if string is `HVAP` and a column is called Run_1, it would be renamed as \n",
    "        \"0_HVAP_noise\".\n",
    "    \"\"\"\n",
    "    df = df.fillna(0)\n",
    "    for col_name in df.columns:\n",
    "        if col_name[:3] == \"Run\":\n",
    "            new_name = str(int(col_name[4:]) - 1) + \"_{}_noise\".format(string)\n",
    "            df = df.rename(columns={col_name: new_name})\n",
    "    return df\n",
    "\n",
    "def build_er_df(dir_name, state_id, filename, precinct_assignments_fp, county_fips):\n",
    "    \"\"\" Builds a CSV that can be fed into plot_elect_grid() to easily make ER plots.\n",
    "        \n",
    "        Args:\n",
    "            dir_name (str) : Filepath where `filename` exists.\n",
    "            state_id (int) : FIPS code of state the runs were run on.\n",
    "            filename (str) : Name of MDF file in `dir_name` to open.\n",
    "            precinct_assignments_fp (str) : Filepath to where the precinct_assignments file is.\n",
    "    \"\"\"\n",
    "    print(\"Collecting races...\")\n",
    "    hisp = collect_by_enumdist(dir_name, state_id, filename, hisp=True, vap=False)\n",
    "    nh_white = collect_by_enumdist(dir_name, state_id, filename, race=1, vap=False)\n",
    "    nh_black = collect_by_enumdist(dir_name, state_id, filename, race=2, vap=False)\n",
    "    nh_amin = collect_by_enumdist(dir_name, state_id, filename, race=3, vap=False)\n",
    "    nh_asian = collect_by_enumdist(dir_name, state_id, filename, race=4, vap=False)\n",
    "    nh_hawaiian = collect_by_enumdist(dir_name, state_id, filename, race=5, vap=False)\n",
    "    nh_other = collect_by_enumdist(dir_name, state_id, filename, race=6, vap=False)\n",
    "    \n",
    "    hvap = collect_by_enumdist(dir_name, state_id, filename, hisp=True, vap=True)\n",
    "    wvap = collect_by_enumdist(dir_name, state_id, filename, race=1, vap=True)\n",
    "    bvap = collect_by_enumdist(dir_name, state_id, filename, race=2, vap=True)\n",
    "    amin_vap = collect_by_enumdist(dir_name, state_id, filename, race=3, vap=True)\n",
    "    asian_vap = collect_by_enumdist(dir_name, state_id, filename, race=4, vap=True)\n",
    "    hawaiian_vap = collect_by_enumdist(dir_name, state_id, filename, race=5, vap=True)\n",
    "    other_vap = collect_by_enumdist(dir_name, state_id, filename, race=6, vap=True)\n",
    "    \n",
    "    vap = collect_by_enumdist(dir_name, state_id, filename, vap=True)\n",
    "    \n",
    "    print(\"Cleaning...\")\n",
    "    # clean\n",
    "    [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, \n",
    "     hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = [clean_df(df) for df in [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]]\n",
    "    \n",
    "    print(\"Merging with precincts...\")\n",
    "    # merge with precincts\n",
    "    precinct_assignments = get_precinct_assignments(precinct_assignments_fp)\n",
    "    \n",
    "    def bell_cameron_processing(df, county_fips):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df[\"State\"] = \"48\"\n",
    "        df[\"County\"] = county_fips\n",
    "        df[\"tract\"] = df[\"GEOID10\"].apply(lambda x: x[5:11])\n",
    "        df[\"tract\"] = df[\"tract\"].apply(lambda x: x[:5])\n",
    "        df[\"tract\"] = df[\"tract\"].str.pad(6, side='left', fillchar='0')\n",
    "        df[\"block\"] = df[\"GEOID10\"].apply(lambda x: x[-4:])\n",
    "        df[\"GEOID10\"] = df[\"State\"] + df[\"County\"] + df[\"tract\"] + df[\"block\"]\n",
    "        df = df.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        df = df[~df[\"Precinct\"].isna()]\n",
    "        return df\n",
    "    \n",
    "    def nueces_processing(df, county_fips):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df[\"State\"] = \"48\"\n",
    "        df[\"County\"] = county_fips\n",
    "        df[\"tract\"] = df[\"GEOID10\"].apply(lambda x: x[5:11])\n",
    "        df[\"tract\"] = df[\"tract\"].apply(lambda x: x[:4])\n",
    "        df[\"tract\"] = df[\"tract\"].str.pad(6, side='left', fillchar='0')\n",
    "\n",
    "        df.loc[(df.tract == '009800'),'tract'] = '980000'\n",
    "        df.loc[(df.tract == '009900'),'tract'] = '990000'\n",
    "        df.loc[(df.tract == '005000'),'tract'] = '000500'\n",
    "        df.loc[(df.tract == '007000'),'tract'] = '000700'\n",
    "        df.loc[(df.tract == '008000'),'tract'] = '000800'\n",
    "        df.loc[(df.tract == '009000'),'tract'] = '000900'\n",
    "\n",
    "        df[\"block\"] = df[\"GEOID10\"].apply(lambda x: x[-4:])\n",
    "        df[\"GEOID10\"] = df[\"State\"] + df[\"County\"] + df[\"tract\"] + df[\"block\"]\n",
    "        df = df.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        df = df[~df[\"Precinct\"].isna()]\n",
    "        return df\n",
    "    \n",
    "    if county_fips == \"355\":\n",
    "        [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, \n",
    "         hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = [nueces_processing(df, county_fips) for df in [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]]\n",
    "    elif county_fips == \"061\" or county_fips == \"027\":\n",
    "        [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, \n",
    "         hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = [bell_cameron_processing(df, county_fips) for df in [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]]\n",
    "    else:\n",
    "        hisp = hisp.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        nh_white = nh_white.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        nh_black = nh_black.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        nh_amin = nh_amin.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        nh_asian = nh_asian.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        nh_hawaiian = nh_hawaiian.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        nh_other = nh_other.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "\n",
    "        hvap = hvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        wvap = wvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        bvap = bvap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        amin_vap = amin_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        asian_vap = asian_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        hawaiian_vap = hawaiian_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        other_vap = other_vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "        vap = vap.merge(precinct_assignments, on=\"GEOID10\", how=\"outer\")\n",
    "\n",
    "    # rename\n",
    "    def rename_multiple_dfs(dfs, names):\n",
    "        assert(len(dfs) == len(names))\n",
    "        \n",
    "        new_dfs = []\n",
    "        for i in range(len(dfs)):\n",
    "            new_dfs.append(rename_cols(dfs[i], names[i]))\n",
    "        \n",
    "        return new_dfs\n",
    "    \n",
    "    dfs = [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]\n",
    "    \n",
    "    names = [\"HISP\", \"NHWHITE\", \"NHBLACK\", \"NHAMIN\", \"NHASIAN\", \"NHHAWAIIAN\", \"NHOTHER\", \n",
    "             \"HVAP\", \"WVAP\", \"BVAP\", \"AMINVAP\", \"ASIANVAP\", \"HAWAIIANVAP\", \"OTHERVAP\", \"VAP\"]\n",
    "    \n",
    "    print(\"Renaming cols...\")\n",
    "    [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = rename_multiple_dfs(dfs, names)\n",
    "    \n",
    "    \n",
    "    # groupby Precincts\n",
    "    dfs = [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]\n",
    "    \n",
    "    def groupby_precincts(dfs):\n",
    "        new_dfs = []\n",
    "        for df in dfs:\n",
    "            new_dfs.append(df.groupby(\"Precinct\").sum())\n",
    "        return new_dfs\n",
    "            \n",
    "    print(\"Grouping by precincts..\")\n",
    "    [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap] = groupby_precincts(dfs)\n",
    "    \n",
    "    dfs = [hisp, nh_white, nh_black, nh_amin, nh_asian, nh_hawaiian, nh_other, hvap, wvap, bvap, amin_vap, asian_vap, hawaiian_vap, other_vap, vap]\n",
    "    \n",
    "    print(\"The big merge...\")\n",
    "    df_merged = reduce(lambda left,right: pd.merge(left, right, how='outer', left_index=True, right_index=True), \n",
    "                       dfs)\n",
    "    return df_merged\n",
    "\n",
    "def get_pops(dir_name, state_id, filename):\n",
    "    \"\"\" Gets the population from each file `filename` in the \"output_\" dirs in `dirname`, \n",
    "        cleans it and returns it in a Pandas DataFrame.\n",
    "        State ID is the FIPS code of the state where the runs are run on.\n",
    "    \"\"\"\n",
    "    pops = collect_by_enumdist(dir_name, state_id, filename)\n",
    "    pops = clean_df(pops)\n",
    "    return pops\n",
    "\n",
    "def save_populations_at_block_level(runs_dirname, \n",
    "                                    zips_arr, \n",
    "                                    county_filenames,\n",
    "                                    state_id=48):\n",
    "    \"\"\" Save the population at the blocks level for the runs in `zips_arr`.\n",
    "        \n",
    "        Args:\n",
    "            runs_dirname (str) : directory where the files in `zips_arr` are stored.\n",
    "            zips_arr (list str): list of zip files that contain the runs \n",
    "            state_id     (int) : FIPS code of state the runs are from. Defaults to 48 for TX\n",
    "            dallas_filename (str) : Name of Dallas MDF file in each output dir\n",
    "    \"\"\"\n",
    "    def bell_cameron_processing(df, county_fips):\n",
    "        df[\"State\"] = \"48\"\n",
    "        df[\"County\"] = county_fips\n",
    "        df[\"tract\"] = df[\"GEOID10\"].apply(lambda x: x[5:11])\n",
    "        df[\"tract\"] = df[\"tract\"].apply(lambda x: x[:5])\n",
    "        df[\"tract\"] = df[\"tract\"].str.pad(6, side='left', fillchar='0')\n",
    "        df[\"block\"] = df[\"GEOID10\"].apply(lambda x: x[-4:])\n",
    "        df[\"GEOID10\"] = df[\"State\"] + df[\"County\"] + df[\"tract\"] + df[\"block\"]\n",
    "        return df\n",
    "    \n",
    "    def nueces_processing(df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df[\"State\"] = \"48\"\n",
    "        df[\"County\"] = \"355\"\n",
    "        df[\"tract\"] = df[\"GEOID10\"].apply(lambda x: x[5:11])\n",
    "        df[\"tract\"] = df[\"tract\"].apply(lambda x: x[:4])\n",
    "        df[\"tract\"] = df[\"tract\"].str.pad(6, side='left', fillchar='0')\n",
    "\n",
    "        df.loc[(df.tract == '009800'),'tract'] = '980000'\n",
    "        df.loc[(df.tract == '009900'),'tract'] = '990000'\n",
    "        df.loc[(df.tract == '005000'),'tract'] = '000500'\n",
    "        df.loc[(df.tract == '007000'),'tract'] = '000700'\n",
    "        df.loc[(df.tract == '008000'),'tract'] = '000800'\n",
    "        df.loc[(df.tract == '009000'),'tract'] = '000900'\n",
    "\n",
    "        df[\"block\"] = df[\"GEOID10\"].apply(lambda x: x[-4:])\n",
    "        df[\"GEOID10\"] = df[\"State\"] + df[\"County\"] + df[\"tract\"] + df[\"block\"]\n",
    "        return df\n",
    "    \n",
    "    for run in zips_arr:\n",
    "        filename = run[:-8]\n",
    "        print(filename)\n",
    "        extract_from_zip(runs_dirname + run, runs_dirname)   \n",
    "        for county in county_filenames:\n",
    "            print(county)\n",
    "            fp = runs_dirname + \"five_counties/\" + run[:-4]\n",
    "            print(fp)\n",
    "            tot_pops = get_pops(fp, state_id, county)\n",
    "            if county == \"Nueces.dat\":\n",
    "                tot_pops = nueces_processing(tot_pops)\n",
    "            elif county == \"Bell.dat\":\n",
    "                tot_pops = bell_cameron_processing(tot_pops, \"027\")\n",
    "            elif county == \"Cameron.dat\":\n",
    "                tot_pops = bell_cameron_processing(tot_pops, \"061\")\n",
    "            tot_pops.to_csv(filename + \"_\" + county + \"_block_pops.csv\")\n",
    "\n",
    "        os.system(\"rm -r ./five_counties_cleaned/five_counties_cleaned\")\n",
    "        os.system(\"rm -r ./five_counties_cleaned/five_counties\")\n",
    "        \n",
    "def build_csvs(runs_dirname, county_filenames, precinct_assignments_fps, state_id, county_fips):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert(len(county_filenames) == len(precinct_assignments_fps))\n",
    "    \n",
    "    for root, dirs, files in os.walk(runs_dirname):\n",
    "        for file in files:\n",
    "            if file[-4:] == \".zip\":\n",
    "                print(file)\n",
    "                texas_fp = root + file\n",
    "                dirname = texas_fp[:-4]\n",
    "                extract_from_zip(texas_fp, root)    \n",
    "                for i in range(len(county_filenames)):\n",
    "                    if county_filenames[i] != \"Cameron.dat\":\n",
    "                        continue\n",
    "                    print(\"Doing \", county_filenames[i][:-4], \" for \", file[:-8])\n",
    "                    df = build_er_df(runs_dirname, \n",
    "                                     state_id, \n",
    "                                     county_filenames[i], \n",
    "                                     precinct_assignments_fps[i],\n",
    "                                     county_fips[i]\n",
    "                                    )\n",
    "                    save_filename = file[:-8] + \"_\" + county_filenames[i][:-4]\n",
    "                    df.to_csv(save_filename + \".csv\")\n",
    "                print(\"Deleting {}\".format(runs_dirname + runs_dirname[:-1]))\n",
    "#                 os.system(\"rm -r \" + runs_dirname + runs_dirname[:-1])\n",
    "                os.system(\"rm -r ./five_counties_cleaned/five_counties_cleaned\")\n",
    "                os.system(\"rm -r ./five_counties_cleaned/five_counties\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_csvs(with_hh_dirname, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_csvs(without_hh_dirname, dallas_filename, precinct_assignments_fp, state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs_with_hhs = ['TEXAS_STUB_HH_mid_1.ini.zip',\n",
    "#                  'TEXAS_STUB_HH_eq_1.ini.zip',\n",
    "#                  'TEXAS_STUB_HH_top_1.ini.zip']\n",
    "\n",
    "# save_populations_at_block_level(\"with_hhs/\", runs_with_hhs, dallas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # five counties pops\n",
    "# runs = [\"five_counties_top.ini.zip\", \n",
    "#         \"five_counties_mid.ini.zip\", \"five_counties_bottom.ini.zip\"]\n",
    "# county_filenames = [\"Nueces.dat\", \"Bell.dat\", \"Cameron.dat\"]\n",
    "\n",
    "# save_populations_at_block_level(\"./five_counties_cleaned/\", runs, county_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # five counties er\n",
    "# five_counties_dir = \"./five_counties_cleaned/\"\n",
    "# county_fips = [\"167\", \"355\", \"027\", \"039\", \"061\"]\n",
    "# county_filenames = [\"Galveston.dat\", \"Nueces.dat\", \"Bell.dat\", \"Brazoria.dat\", \"Cameron.dat\"]\n",
    "# precinct_assignments_fps = [\"galveston_assignments.csv\", \"nueces_assignments.csv\", \"bell_assignments.csv\",\n",
    "#                             \"brazoria_assignments.csv\", \"cameron_assignments.csv\"]\n",
    "# state_id = 48\n",
    "\n",
    "# build_csvs(five_counties_dir, county_filenames, precinct_assignments_fps, state_id, county_fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\" Some simple cleanups on `df` to ready it to make ER csvs.\n",
    "    \"\"\"\n",
    "    df[\"Enumdist\"] = df[\"Enumdist\"].astype(str).str.pad(width=11, side='left', fillchar='0')\n",
    "    df[\"County\"] = df[\"County\"].astype(str).str.pad(width=3, side='left', fillchar='0')\n",
    "    df[\"GEOID10\"] = df[\"State\"].astype(str) + df[\"County\"] + df[\"Enumdist\"]\n",
    "    df[\"GEOID10\"] = df[\"GEOID10\"].str[:11] + df[\"GEOID10\"].str[-4:]\n",
    "    df = df.drop(columns=[\"State\"])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_pops(dir_name, state_id, filename):\n",
    "    \"\"\" Gets the population from each file `filename` in the \"output_\" dirs in `dirname`, \n",
    "        cleans it and returns it in a Pandas DataFrame.\n",
    "        State ID is the FIPS code of the state where the runs are run on.\n",
    "    \"\"\"\n",
    "    pops = collect_by_enumdist(dir_name, state_id, filename)\n",
    "    pops = clean_df(pops)\n",
    "    return pops\n",
    "\n",
    "def save_populations_at_block_level(runs_dirname, \n",
    "                                    zips_arr, \n",
    "                                    county_filenames,\n",
    "                                    state_id=48):\n",
    "    \"\"\" Save the population at the blocks level for the runs in `zips_arr`.\n",
    "        \n",
    "        Args:\n",
    "            runs_dirname (str) : directory where the files in `zips_arr` are stored.\n",
    "            zips_arr (list str): list of zip files that contain the runs \n",
    "            state_id     (int) : FIPS code of state the runs are from. Defaults to 48 for TX\n",
    "            dallas_filename (str) : Name of Dallas MDF file in each output dir\n",
    "    \"\"\"\n",
    "    for run in zips_arr:\n",
    "        filename = run[:-8]\n",
    "        print(filename)\n",
    "#         extract_from_zip(runs_dirname + run, runs_dirname)   \n",
    "        for county in county_filenames:\n",
    "            print(county)\n",
    "#             fp = runs_dirname + runs_dirname + run[:-4]\n",
    "            fp = runs_dirname + \"remaining_runs/\" + run[:-4]\n",
    "            print(fp)\n",
    "            tot_pops = get_pops(fp, state_id, county)\n",
    "            tot_pops.to_csv(filename + \"_\" + county + \"_block_pops.csv\", index=False)\n",
    "\n",
    "        os.system(\"rm -r \" + runs_dirname + runs_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXAS_STUB_bottom_1\n",
      "Dallas.dat\n",
      "./without_hhs/remaining_runs/TEXAS_STUB_bottom_1.ini\n"
     ]
    }
   ],
   "source": [
    "runs = ['TEXAS_STUB_bottom_1.ini.zip']\n",
    "# runs = ['TEXAS_STUB_eq_1.ini.zip',\n",
    "#         'TEXAS_STUB_bottom_1.ini.zip',\n",
    "#         'TEXAS_STUB_mid_1.ini.zip',]\n",
    "county_filenames = [\"Dallas.dat\"]\n",
    "\n",
    "save_populations_at_block_level(\"./without_hhs/\", runs, county_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
